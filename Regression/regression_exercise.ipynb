{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42834217",
   "metadata": {},
   "source": [
    "# Exercise on the use of OLS, Lasso, Ridge and PCR regression\n",
    "\n",
    "In this exercise we'll check the difference between some regression algorithms.\n",
    "\n",
    "The goal is to predict a measure of the progression of diabetes from some input features, such as age, body weight, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b988fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "features = data.feature_names\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Split the data into train and test\n",
    "# - Center and scale the data\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254607",
   "metadata": {},
   "source": [
    "## Ordinary Least-Squares regression\n",
    "\n",
    "A general regression problem can be written as: $y = f(\\mathbf{x})$  with $y \\in \\mathbb{R}$, $x \\in \\mathbb{R}^d$ and $f: \\mathbb{R}^d \\mapsto \\mathbb{R}$.\n",
    "In linear regression, the function is represented by an array of weights: $y = \\mathbf{w}^T \\mathbf{x} = \\sum_{i}^d w_i x_i$.\n",
    "\n",
    "We need to tune the weights to our process, so we collect some data on the inputs $\\mathbf{x}$ and the target $y$. The objective is to tune the weights to minimize the euclidean distance between the observations $\\mathbf{y} \\in \\mathbb{R}^n$ and the predictions $\\mathbf{X} \\mathbf{w}$:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: \n",
    "# - Fit a Ordinart least squares regression model\n",
    "# - Do a parity plot (observed vs predicted)\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f133f0",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To assess the quality of the regression model, we need to compare the predictions and observations for some points that were not used to train the model.\n",
    "There are a huge number of different metrics that can be used depending on the case.\n",
    "Some popular ones are the coefficient of determination $R^2$ and the (root) mean squared error (R)MSE:\n",
    "\\begin{equation}\n",
    "R^2(\\mathbf{y}, \\hat{\\mathbf{y}}) = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{RMSE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "\\end{equation}\n",
    "\n",
    "An overview of error metrics can be found here: https://scikit-learn.org/stable/modules/model_evaluation.html#which-scoring-function-should-i-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Compute the r2, mse and rmse\n",
    "\n",
    "# from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31db41",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "\n",
    "In the OLS regression model we have included all the input features. However, if some features are not correlated with the output this can decrease the accuracy of the model. The LASSO regression model penalizes the coefficients that are different from zero, forcing the weights to be active only if they improve the model.\n",
    "\n",
    "The objective function of the LASSO regression problem is:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} \\frac{1}{2 n} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2 + \\alpha ||\\mathbf{w}||_1$\n",
    "\n",
    "In which the coefficient $\\alpha$ controls how much we regularize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5a07e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Compute the Lasso regression with a random alpha\n",
    "# - Print the coefficients of the OLS and Lasso models\n",
    "# - Print the r2 score of the the models\n",
    "\n",
    "# from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b2a1a",
   "metadata": {},
   "source": [
    "The penalty on the L1 norm is used to promote the sparsity of the regression weights.\n",
    "\n",
    "To infer the correct value of $\\alpha$ to apply we can use the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3e6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - use LassoCV to find the optimal alpha\n",
    "# - compare the models\n",
    "\n",
    "# from sklearn.linear_model import LassoCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aaec55",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "In Ridge regression, the regularization is applied to the $l_2$ norm of the weights. We want to reduce the magnitude of the weights, so that the model is less sensitive to noise.\n",
    "The objective function of a Ridge regression problem is:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2 + \\alpha ||\\mathbf{w}||_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Use RidgeCV to fit the ridge model\n",
    "\n",
    "# from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f638b65",
   "metadata": {},
   "source": [
    "## Principal components regression\n",
    "\n",
    "The principal component regression is the same as the OLS regression, with an extra-step: the PCA is applied to the X matrix, and the linear regression is performed on the new projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf3065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Compute the PCA transform of the data (X)\n",
    "# - Fit a regression model on the transformed data (Z)\n",
    "# - Compare the R2 score with OLS\n",
    "\n",
    "# from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4057ffd",
   "metadata": {},
   "source": [
    "This added step has two benefits:\n",
    "\n",
    "* The features become uncorrelated between them.\n",
    "* The dimensionality of the feature matrix can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - Do the same as in the previous cell using only 5 PCs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
