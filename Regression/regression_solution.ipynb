{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42834217",
   "metadata": {},
   "source": [
    "# Exercise on the use of OLS, Lasso, Ridge and PCR regression\n",
    "\n",
    "In this exercise we'll check the difference between some regression algorithms.\n",
    "\n",
    "The goal is to predict a measure of the progression of diabetes from some input features, such as age, body weight, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b988fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "features = data.feature_names\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# scale and center the data\n",
    "scalerX = StandardScaler()\n",
    "\n",
    "X0_train = scalerX.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254607",
   "metadata": {},
   "source": [
    "## Least-Squares regression\n",
    "\n",
    "A general regression problem can be written as: $y = f(\\mathbf{x})$  with $y \\in \\mathbb{R}$, $x \\in \\mathbb{R}^d$ and $f: \\mathbb{R}^d \\mapsto \\mathbb{R}$.\n",
    "In linear regression, the function is represented by an array of weights: $y = \\mathbf{w}^T \\mathbf{x} = \\sum_{i}^d w_i x_i$.\n",
    "\n",
    "We need to tune the weights to our process, so we collect some data on the inputs $\\mathbf{x}$ and the target $y$. The objective is to tune the weights to minimize the euclidean distance between the observations $\\mathbf{y} \\in \\mathbb{R}^n$ and the predictions $\\mathbf{X} \\mathbf{w}$:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the linear regression object\n",
    "OLS_reg = LinearRegression().fit(X0_train, y_train)\n",
    "\n",
    "# To test the regression, we need to scale and center also the test data\n",
    "X0_test = scalerX.transform(X_test)\n",
    "y_pred_OLS = OLS_reg.predict(X0_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred_OLS)\n",
    "plt.plot(y_test, y_test, c='r', alpha=0.6, ls='--')\n",
    "plt.xlim(y_test.min()-5, y_test.max()+5)\n",
    "plt.ylim(y_test.min()-5, y_test.max()+5)\n",
    "plt.xlabel('Observed target')\n",
    "plt.ylabel('Predicted target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f133f0",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To assess the quality of the regression model, we need to compare the predictions and observations for some points that were not used to train the model.\n",
    "There are a huge number of different metrics that can be used depending on the case.\n",
    "Some popular ones are the coefficient of determination $R^2$ and the (root) mean squared error (R)MSE:\n",
    "\\begin{equation}\n",
    "R^2(\\mathbf{y}, \\hat{\\mathbf{y}}) = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{RMSE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "\\end{equation}\n",
    "\n",
    "An overview of error metrics can be found here: https://scikit-learn.org/stable/modules/model_evaluation.html#which-scoring-function-should-i-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "r2_ols = r2_score(y_test, y_pred_OLS)\n",
    "mse_ols = mean_squared_error(y_test, y_pred_OLS)\n",
    "rmse_ols = root_mean_squared_error(y_test, y_pred_OLS)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'MSE for OLS is: {mse_ols:.2f}')\n",
    "print(f'RMSE for OLS is: {rmse_ols:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31db41",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "\n",
    "In the OLS regression model we have included all the input features. However, if some features are not correlated with the output this can decrease the accuracy of the model. The LASSO regression model penalizes the coefficients that are different from zero, forcing the weights to be active only if they improve the model.\n",
    "\n",
    "The objective function of the LASSO regression problem is:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} \\frac{1}{2 n} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2 + \\alpha ||\\mathbf{w}||_1$\n",
    "\n",
    "In which the coefficient $\\alpha$ controls how much we regularize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5a07e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "Lasso_reg = Lasso(alpha=1.).fit(X0_train, y_train)\n",
    "y_pred_lasso = Lasso_reg.predict(X0_test)\n",
    "\n",
    "print('LS coefficients: ')\n",
    "print(np.round(OLS_reg.coef_, 3))\n",
    "\n",
    "print('Lasso coefficients: ')\n",
    "print(np.round(Lasso_reg.coef_, 3))\n",
    "\n",
    "r2_ols = r2_score(y_test, y_pred_OLS)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'R2 for Lasso is: {r2_lasso:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b2a1a",
   "metadata": {},
   "source": [
    "The penalty on the L1 norm is used to promote the sparsity of the regression weights.\n",
    "\n",
    "To infer the correct value of $\\alpha$ to apply we can use the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3e6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "LassoCV_reg = LassoCV(cv=5, random_state=42).fit(X0_train, y_train)\n",
    "y_pred_lassoCV = LassoCV_reg.predict(X0_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred_lassoCV)\n",
    "plt.plot(y_test, y_test, c='r', alpha=0.6, ls='--')\n",
    "plt.xlim(y_test.min()-1, y_test.max()+1)\n",
    "plt.ylim(y_test.min()-1, y_test.max()+1)\n",
    "plt.xlabel('Observed target')\n",
    "plt.ylabel('Predicted target')\n",
    "plt.show()\n",
    "\n",
    "print('LS coefficients: ')\n",
    "print(np.round(OLS_reg.coef_, 3))\n",
    "\n",
    "print('LassoCV coefficients: ')\n",
    "print(np.round(LassoCV_reg.coef_, 3))\n",
    "\n",
    "r2_lassoCV = r2_score(y_test, y_pred_lassoCV)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'R2 for LassoCV is: {r2_lassoCV:.2f}')\n",
    "\n",
    "print(f'alpha = {LassoCV_reg.alpha_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aaec55",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "In Ridge regression, the regularization is applied to the $l_2$ norm of the weights. We want to reduce the magnitude of the weights, so that the model is less sensitive to noise.\n",
    "The objective function of a Ridge regression problem is:\n",
    "\n",
    "$\\mathbf{w} = \\underset{\\mathbf{w}}{\\mathrm{min}} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2 + \\alpha ||\\mathbf{w}||_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "RidgeCV_reg = RidgeCV(alphas=(0.1, 0.5, 1, 5, 10, 50), cv=5).fit(X0_train, y_train)\n",
    "y_pred_RidgeCV = RidgeCV_reg.predict(X0_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred_RidgeCV)\n",
    "plt.plot(y_test, y_test, c='r', alpha=0.6, ls='--')\n",
    "plt.xlim(y_test.min()-1, y_test.max()+1)\n",
    "plt.ylim(y_test.min()-1, y_test.max()+1)\n",
    "plt.xlabel('Observed target')\n",
    "plt.ylabel('Predicted target')\n",
    "plt.show()\n",
    "\n",
    "print('OLS coefficients: ')\n",
    "print(np.round(OLS_reg.coef_, 3))\n",
    "\n",
    "print('RidgeCV coefficients: ')\n",
    "print(np.round(RidgeCV_reg.coef_, 3))\n",
    "\n",
    "r2_ridgeCV = r2_score(y_test, y_pred_RidgeCV)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'R2 for RidgeCV is: {r2_ridgeCV:.2f}')\n",
    "\n",
    "print(f'alpha = {RidgeCV_reg.alpha_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f638b65",
   "metadata": {},
   "source": [
    "## Principal components regression\n",
    "\n",
    "The principal component regression is the same as the OLS regression, with an extra-step: the PCA is applied to the X matrix, and the linear regression is performed on the new projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf3065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(X0_train)\n",
    "\n",
    "plt.scatter(np.arange(X.shape[1]), pca.explained_variance_ratio_*100)\n",
    "plt.xlabel('PCs')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5b376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_train = pca.components_.T\n",
    "Z_train = X0_train @ A_train\n",
    "Z_test = X0_test @ A_train\n",
    "\n",
    "# The regressio has to be applied to the PC scores\n",
    "PCR_reg = LinearRegression().fit(Z_train, y_train)\n",
    "y_pred_PCR = PCR_reg.predict(Z_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred_PCR)\n",
    "plt.plot(y_test, y_test, c='r', alpha=0.6, ls='--')\n",
    "plt.xlim(y_test.min()-1, y_test.max()+1)\n",
    "plt.ylim(y_test.min()-1, y_test.max()+1)\n",
    "plt.xlabel('Observed log PSA')\n",
    "plt.ylabel('Predicted log PSA')\n",
    "plt.show()\n",
    "\n",
    "print('LS coefficients: ')\n",
    "print(np.round(OLS_reg.coef_, 3))\n",
    "\n",
    "print('PCR coefficients: ')\n",
    "print(np.round(PCR_reg.coef_, 3))\n",
    "\n",
    "r2_pcr = r2_score(y_test, y_pred_PCR)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'R2 for PCR is: {r2_pcr:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4057ffd",
   "metadata": {},
   "source": [
    "This added step has two benefits:\n",
    "\n",
    "* The features become uncorrelated between them.\n",
    "* The dimensionality of the feature matrix can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can test the regression with fewer features\n",
    "q = 5\n",
    "Z_train = X0_train @ A_train[:,:q]\n",
    "Z_test = X0_test @ A_train[:,:q]\n",
    "\n",
    "PCR_reg = LinearRegression().fit(Z_train, y_train)\n",
    "y_pred_PCR = PCR_reg.predict(Z_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred_PCR)\n",
    "plt.plot(y_test, y_test, c='r', alpha=0.6, ls='--')\n",
    "plt.xlim(y_test.min()-1, y_test.max()+1)\n",
    "plt.ylim(y_test.min()-1, y_test.max()+1)\n",
    "plt.xlabel('Observed target')\n",
    "plt.ylabel('Predicted target')\n",
    "plt.show()\n",
    "\n",
    "r2_pcr = r2_score(y_test, y_pred_PCR)\n",
    "\n",
    "print(f'R2 for OLS is: {r2_ols:.2f}')\n",
    "print(f'R2 for PCR is: {r2_pcr:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
