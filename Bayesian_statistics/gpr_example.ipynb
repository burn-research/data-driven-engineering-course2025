{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use GPyTorch to build a Gaussian process regression model\n",
    "\n",
    "This notebook illustrates how to use [GPyTorch](https://gpytorch.ai/) to build a GPR model. GPyTorch is built on top of PyTorch, and it exploits the optimization functions, such as gradient descent, that can be used to train the hyperparameters. By using PyTorch tensors, it also supports GPU acceleration.\n",
    "\n",
    "## Start from some toy data\n",
    "\n",
    "We sample some data points from the function $y = x\\sin(x)$ in the range $x \\in [0, 4\\pi]$, and we add some Gaussian noise with zero mean and variance equal to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{amsmath}')\n",
    "mpl.rc('font',**{'family':'serif','serif':['Palatino'], 'size':14})\n",
    "\n",
    "def toy_function(x):\n",
    "    return x*torch.sin(x)\n",
    "\n",
    "x = torch.linspace(0, 4*np.pi, 100)\n",
    "y_true = toy_function(x)\n",
    "\n",
    "n_train = 20\n",
    "x_train = torch.linspace(0.5*torch.pi, 3.5*torch.pi, n_train)\n",
    "torch.manual_seed(54)\n",
    "noise = torch.normal(0, 1, size=(n_train,))\n",
    "y_train = toy_function(x_train) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y_true, x_train, y_train, filename=''):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "    ax.plot(x, y_true, c='k', alpha=1., ls='-', label='true')\n",
    "    ax.errorbar(x_train, y_train, 2., c='k', fmt='o', label='observed')\n",
    "\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$y$')\n",
    "    ax.legend()\n",
    "    \n",
    "    if filename != '':\n",
    "        fig.savefig(filename, transparent=False, dpi=600, bbox_inches='tight')    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_data(x, y_true, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the class for our GP model\n",
    "\n",
    "As in PyTorch, we need to create a class that inherits from the general ExactGP model. We need also to include a function ```forward(self, x)``` which prescribes the operations that we need to carry out when we apply the model on some inputs:\n",
    "```y = model(x)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "# create a class from the ExactGP template\n",
    "class ModelGP(gpytorch.models.ExactGP):\n",
    "    \n",
    "    # We need to provide:\n",
    "    # - the training input and output (X_train, y_train)\n",
    "    # - the likelihood\n",
    "    # - the mean function\n",
    "    # - the kernel function\n",
    "    def __init__(self, X_train, y_train, likelihood, mean, kernel):\n",
    "        super().__init__(X_train, y_train, likelihood)\n",
    "        \n",
    "        # we store the mean and kernel functions as attributes\n",
    "        self.mean_module = mean\n",
    "        self.covar_module = kernel\n",
    "    \n",
    "    # As for PyTorch, we need to define a forward function which \n",
    "    # transforms the input of the model\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        kernel_x = self.covar_module(x)\n",
    "        # the forward function computes the multivariate normal\n",
    "        # given the mean and the kernel\n",
    "        return MultivariateNormal(mean_x, kernel_x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with a simple neural network in PyTorch\n",
    "\n",
    "As a comparison, this is how a PyTorch neural network is built:\n",
    "\n",
    "```python\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size) # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.means import ZeroMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "mean = ZeroMean() # zero mean on the prior\n",
    "\n",
    "# The kernel is the RBF kernel with a scale parameter\n",
    "kernel = ScaleKernel(RBFKernel()) \n",
    "\n",
    "# We set a Gaussian learnable noise\n",
    "likelihood = GaussianLikelihood()\n",
    "\n",
    "# Create the model\n",
    "model = ModelGP(x_train, y_train, likelihood, mean, kernel)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Send the data and the model to the device\n",
    "x_train.to(device)\n",
    "y_train.to(device)\n",
    "model.to(device);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_grad is used to avoid the computation of \n",
    "# the gradient for optimization\n",
    "with torch.no_grad():\n",
    "    output = model(x_train)\n",
    "\n",
    "# check the mean and stddev of the prior\n",
    "print(output)\n",
    "print(output.mean)\n",
    "print(output.stddev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the model in train(), to find the optimal parameters\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# the optimizer is taken straight from PyTorch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n",
    "\n",
    "# as loss function, we use the log marginal likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "n_iterations = 100\n",
    "for i in range(n_iterations):\n",
    "    optimizer.zero_grad() # set the gradient to zero\n",
    "    output = model(x_train) # compute the output of the model\n",
    "    loss = -mll(output, y_train) # compare the output with the training data\n",
    "    loss.backward() # compute backward propagation on the noise and \n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, n_iterations, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_bounds(x, y_true, y_mean, y_bounds, x_train, y_train, filename=''):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "    ax.plot(x, y_mean, c='k', alpha=.7, ls='--', label='mean')\n",
    "    ax.plot(x, y_true, c='k', alpha=.7, ls='-', label='true')\n",
    "    ax.fill_between(x, y_bounds[0], y_bounds[1], alpha=0.3)\n",
    "    ax.errorbar(x_train, y_train, 2., c='k', fmt='o', label='observed')\n",
    "\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$y$')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_samples(x, y_true, y_samples, x_train, y_train, filename=''):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "    for i, y in enumerate(y_samples):\n",
    "        ax.plot(x, y, c='k', alpha=.05, ls='-')\n",
    "\n",
    "    ax.plot(x, y_true, c='k', alpha=1., ls='-', label='true')\n",
    "    ax.errorbar(x_train, y_train, 2., c='k', fmt='o', label='observed')\n",
    "\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$y$')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# we switch to the predictive posterior\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # get the predictive posterior\n",
    "    prediction = model(x)\n",
    "    # sample from the predictive posterior\n",
    "    y_samples = prediction.sample(torch.Size([100])).numpy()\n",
    "    # get the mean\n",
    "    y_pred = prediction.mean.numpy()\n",
    "    # get the confidence interval\n",
    "    y_low, y_up = prediction.confidence_region()\n",
    "\n",
    "plot_prediction_bounds(x, y_true, y_pred, [y_low.numpy(), y_up.numpy()], x_train, y_train)\n",
    "plot_prediction_samples(x, y_true, y_samples, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known noise\n",
    "\n",
    "What if we already know the noise of the measurements, for example if we have the uncertainty of the equipment?\n",
    "We can view it as a fixed, non-trainable parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.likelihoods import FixedNoiseGaussianLikelihood\n",
    "\n",
    "mean = ZeroMean()\n",
    "kernel = ScaleKernel(RBFKernel()) \n",
    "\n",
    "# We set a fixed Gaussian noise\n",
    "noise = 1e-3*torch.ones(n_train)\n",
    "likelihood = FixedNoiseGaussianLikelihood(noise)\n",
    "\n",
    "# Create the model\n",
    "model = ModelGP(x_train, y_train, likelihood, mean, kernel)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Send the data and the model to the device\n",
    "x_train.to(device)\n",
    "y_train.to(device)\n",
    "model.to(device);\n",
    "\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "n_iterations = 100\n",
    "for i in range(n_iterations):\n",
    "    optimizer.zero_grad() # set the gradient to zero\n",
    "    output = model(x_train) # compute the output of the model\n",
    "    loss = -mll(output, y_train) # compare the output with the training data\n",
    "    loss.backward() # compute backward propagation on the noise and \n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(x)\n",
    "    y_samples = prediction.sample(torch.Size([100])).numpy()\n",
    "    y_pred = prediction.mean.numpy()\n",
    "    y_low, y_up = prediction.confidence_region()\n",
    "\n",
    "plot_prediction_bounds(x, y_true, y_pred, [y_low.numpy(), y_up.numpy()], x_train, y_train)\n",
    "plot_prediction_samples(x, y_true, y_samples, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kernels\n",
    "\n",
    "The choice of the kernel controls the type of model we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import LinearKernel, PolynomialKernel, CosineKernel, MaternKernel, ConstantKernel\n",
    "\n",
    "# change the kernel\n",
    "kernel = ScaleKernel(LinearKernel()) \n",
    "# kernel = ScaleKernel(PolynomialKernel(2)) \n",
    "# kernel = ScaleKernel(CosineKernel())\n",
    "\n",
    "# period_cst = Interval(0, 4*np.pi)\n",
    "# kernel = ScaleKernel(CosineKernel(period_length_constraint=period_cst)) \n",
    "# kernel = ScaleKernel(LinearKernel() * CosineKernel(period_length_constraint=period_cst)) \n",
    "\n",
    "mean = ZeroMean()\n",
    "\n",
    "noise = 1.*torch.ones(n_train)\n",
    "likelihood = FixedNoiseGaussianLikelihood(noise)\n",
    "\n",
    "model = ModelGP(x_train, y_train, likelihood, mean, kernel)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_train.to(device)\n",
    "y_train.to(device)\n",
    "model.to(device);\n",
    "\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "n_iterations = 100\n",
    "for i in range(n_iterations):\n",
    "    optimizer.zero_grad() # set the gradient to zero\n",
    "    output = model(x_train) # compute the output of the model\n",
    "    loss = -mll(output, y_train) # compare the output with the training data\n",
    "    loss.backward() # compute backward propagation on the noise and \n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(x)\n",
    "    y_samples = prediction.sample(torch.Size([100])).numpy()\n",
    "    y_pred = prediction.mean.numpy()\n",
    "    y_low, y_up = prediction.confidence_region()\n",
    "\n",
    "plot_prediction_bounds(x, y_true, y_pred, [y_low.numpy(), y_up.numpy()], x_train, y_train)\n",
    "plot_prediction_samples(x, y_true, y_samples, x_train, y_train)\n",
    "\n",
    "if isinstance(model.covar_module.base_kernel, CosineKernel):\n",
    "    print(model.covar_module.base_kernel.period_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
